---
title: 2.1 逻辑回归的代价函数
# icon: pen-to-square
date: 2024-10-21
category:
  - 机器学习
tag:
  - 机器学习
---

## 训练集

![image-20241021171257226](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/2.1_Cost_Function_for_Logistic_Regression.assests/image-20241021171257226.png)

## 怎么选择w,b?

### 平方误差成本

![image-20241021171751325](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/2.1_Cost_Function_for_Logistic_Regression.assests/image-20241021171751325.png)

![image-20241021204645428](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/2.1_Cost_Function_for_Logistic_Regression.assests/image-20241021204645428.png)

### 成本函数

$$
J(\vec{w}, b) = \frac{1}{m}\displaystyle \sum^{m}_{i = 0}{\frac{1}{2}}(f_{\vec{w},b}(\vec{x}^{(i)}) - y^{(i)})
$$

该成本函数并不是一个好的选择。

### 逻辑损失函数

![image-20241021173247537](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/2.1_Cost_Function_for_Logistic_Regression.assests/image-20241021173247537.png)

$$
\begin{equation}
L(f_{\vec{w}, b}(\vec{x}^{(i)}, y^{(i)})=\left\{
\begin{aligned}
-\log(f_{\vec{w}, b}(\vec{x}^{(i)})) & \quad \text{当 } y^{(i)} = 1\\ 
-\log(1-f_{\vec{w}, b}(\vec{x}^{(i)})) & \quad \text{当 } y^{(i)} = 0\\
\end{aligned}
\right.
\end{equation}
$$

#### 为什么起作用？

![image-20241021204801950](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/2.1_Cost_Function_for_Logistic_Regression.assests/image-20241021204801950.png)

![image-20241021204841499](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/2.1_Cost_Function_for_Logistic_Regression.assests/image-20241021204841499.png)
