---
title: 3.1 实现梯度下降
# icon: pen-to-square
date: 2024-10-21
category:
  - 机器学习
tag:
  - 机器学习
---

## 训练逻辑回归

![image-20241021212913443](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/3.1_Gradient_Descent_Implementation.assests/image-20241021212913443.png)

找到
$$
\vec{w}, b
$$
给一个新的
$$
\vec{x}
$$
输出
$$
f_{\vec{w}, b}(\vec{x}) = \frac{1}{1 + e^{-(\vec{w} \cdot \vec{x} + b)}}
$$
预测概率：
$$
p(y = 1 \mid \vec{x};\vec{w},b)
$$

## 梯度下降

![image-20241021213640009](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/3.1_Gradient_Descent_Implementation.assests/image-20241021213640009.png)

成本函数
$$
J(\vec{w},b) = - \frac{1}{m} \displaystyle \sum^{m}_{i=1}[ y^{(i)}\log(f_{\vec{w},b}(x^{(i)}))+(1-y^{(i)})\log(1-f_{\vec{w}, b}(\vec{x}^{(i)}))]
$$
梯度下降：
$$
w_j = w_j - \alpha \frac{\partial}{\partial w_j}J(\vec{w}, b)
$$

$$
b = b - \alpha \frac{\partial}{\partial b}J(\vec{w}, b)
$$

注意也要同时更新w,b

其中
$$
\frac{\partial}{\partial w_j}J(\vec{w}, b) = \frac{1}{m} \displaystyle \sum^{m}_{i=1}(f_{\vec{w}, b}(\vec{x}^{(i)}-y^{(i)})x_j^{(i)}
$$

$$
\frac{\partial}{\partial b}J(\vec{w}, b) = \frac{1}{m} \displaystyle \sum^{m}_{i=1}(f_{\vec{w}, b}(\vec{x}^{(i)}-y^{(i)})
$$

![image-20241022105420468](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/3.1_Gradient_Descent_Implementation.assests/image-20241022105420468.png)

线性回归：
$$
f_{\vec{w}, b}(x) = \vec{w} \cdot \vec{x} + b
$$
逻辑回归：
$$
f_{\vec{w}, b}(\vec{x}) = \frac{1}{1 + e^{(-\vec{w} \cdot \vec{x} + b)}}
$$
