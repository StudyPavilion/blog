---
title: 4.4 用于逻辑回归的正则方法
# icon: pen-to-square
date: 2024-10-26
category:
  - 机器学习
tag:
  - 机器学习
---

## 成本函数

![image-20241026202744233](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/4.5.assests/image-20241026202744233.png)
$$
J(\vec{w}, b)= - \frac{1}{m} \displaystyle \sum^{m}_{i=1}[ y^{(i)}\log(f_{\vec{w},b}(x^{(i)}))+(1-y^{(i)})\log(1-f_{\vec{w}, b}(\vec{x}^{(i)}))] + \frac{\lambda}{2m} \displaystyle \sum^n_{j=1}w^2_j
$$

## 梯度下降

![image-20241026203624478](./../../../.vuepress/public/assets/images/Machine_learning/lesson_one/week_three/4.5_Regularized_logistic_regression.assests/image-20241026203624478.png)
$$
w_j = w_j - \alpha \frac{\partial}{\partial w_j}J(\vec{w}, b)
$$

$$
b = b - \alpha \frac{\partial}{\partial b}J(\vec{w}, b)
$$

其中
$$
\frac{\partial}{\partial b}J(\vec{w}, b) = \frac{1}{m} \displaystyle \sum^{m}_{i=1}(f_{\vec{w}, b}(\vec{x}^{(i)}-y^{(i)})x_j^{(i)}
$$

$$
\frac{\partial}{\partial b}J(\vec{w}, b) = \frac{1}{m} \displaystyle \sum^{m}_{i=1}(f_{\vec{w}, b}(\vec{x}^{(i)}-y^{(i)})
$$
